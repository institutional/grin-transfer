# GRIN-to-S3

An async pipeline for extracting Google Books data from GRIN (Google Return Interface) to local databases and S3-compatible storage, designed for academic libraries.

## Requirements

- **Python 3.12+** (required for modern language features like match/case statements and union types)
- Dependencies managed via `pyproject.toml`

## Quick Start

```bash
# Install in development mode (includes all dependencies and dev tools)
pip install -e ".[dev]"

# Set up OAuth2 credentials (interactive)
python auth.py setup

# Collect books to local database with named run
python collect_books.py --run-name "my_collection"

# Enrich with detailed GRIN metadata
python grin_enrichment.py enrich output/my_collection/books.db

# Export to CSV
python grin_enrichment.py export-csv output/my_collection/books.db --output books.csv
```

## OAuth2 Setup

The `auth.py setup` command walks you through:

1. **Client Configuration**: Creates Google Cloud OAuth2 application
2. **Authorization Flow**: Browser-based Google authentication  
3. **Credential Storage**: Secure JSON credential files
4. **Testing**: Verifies authentication works with GRIN

### Required Files
Credentials are stored in `~/.config/grin-to-s3/`:
- `client_secret.json` or `secrets.json` - OAuth2 client configuration (from Google Cloud Console)
- `credentials.json` or `token.json` - User authentication tokens (generated by setup)
- `r2_credentials.json` - Cloudflare R2 credentials (optional, for R2 storage)

## Cloudflare R2 Setup (Optional)

To use Cloudflare R2 for storage, create an R2 credentials file:

```bash
# Create the config directory if it doesn't exist
mkdir -p ~/.config/grin-to-s3

# Copy template and edit with your credentials
cp r2-credentials-template.json ~/.config/grin-to-s3/r2_credentials.json
# Edit the file with your actual R2 credentials
```

**Getting R2 Credentials:**
1. Go to Cloudflare Dashboard â†’ R2 Object Storage
2. Create an R2 API token with Object Read and Write permissions
3. Note your Account ID from the sidebar
4. Use the generated Access Key ID and Secret Access Key

## Book Collection Pipeline

The book collection system uses a three-stage pipeline:

1. **Collection**: Gather book metadata into SQLite database
2. **Enrichment**: Add detailed GRIN metadata fields  
3. **Export**: Generate CSV files for analysis

### Stage 1: Book Collection

```bash
# Basic collection with named run
python collect_books.py --run-name "harvard_2024"

# Test mode (no network calls, uses mock data)
python collect_books.py --test-mode --limit 100

# With rate limiting (max 5 QPS to respect API limits)
python collect_books.py --run-name "collection" --rate-limit 2.0 --limit 1000
```

### Collection Options

```bash
# Custom pagination for large collections
python collect_books.py --run-name "large_collection" --page-size 5000 --max-pages 2000

# With storage backend integration
python collect_books.py --run-name "s3_check" --storage s3 --storage-config bucket=my-bucket

# Debug mode with detailed logging  
python collect_books.py --run-name "debug_run" --log-level DEBUG
```

**Key Parameters:**
- `--rate-limit`: API requests per second (default: 5.0, max recommended)
- `--page-size`: Books per API request (default: 5000)
- `--max-pages`: Maximum pages to process (default: 1000)
- `--limit`: Total books to collect (useful for testing)

### Stage 2: GRIN Metadata Enrichment

After collecting books, enrich them with detailed GRIN metadata:

```bash
# Check enrichment status
python grin_enrichment.py status output/harvard_2024/books.db

# Start enrichment process (optimized for maximum throughput)
python grin_enrichment.py enrich output/harvard_2024/books.db

# Custom enrichment settings
python grin_enrichment.py enrich output/harvard_2024/books.db \
  --batch-size 5000 \
  --rate-limit 0.2 \
  --max-concurrent 5 \
  --limit 10000
```

**Enrichment Features:**
- **Dynamic batch sizing**: Automatically uses maximum URL capacity (~1000+ barcodes per request)
- **Concurrent processing**: Up to 5 simultaneous API requests with rate limiting
- **Resume capability**: Automatically resumes from interruptions
- **ETA calculation**: Shows estimated completion time after initial batches
- **Comprehensive logging**: Detailed file logs with minimal console output

### Stage 3: Data Export

```bash
# Export enriched data to CSV
python grin_enrichment.py export-csv output/harvard_2024/books.db --output books_enriched.csv
```

### Output Data Fields

The enriched dataset includes:
- **Core**: Barcode, Title, GRIN State, Processing State
- **GRIN Timestamps**: Scanned, Converted, Downloaded, Processed, Analyzed dates
- **Quality Metrics**: Material Error %, Overall Error %, OCR Analysis Score, OCR GTD Score
- **Metadata**: Viewability, Conditions, Scannable, Digitization Method, Tagging, Audit
- **Availability**: Opted-out status, claimed status
- **Tracking**: Collection and enrichment timestamps

## Resume Functionality

Both collection and enrichment automatically save progress:
- Progress files stored in run-specific directories (`output/{run_name}/`)
- SQLite database tracks processed books and enrichment status
- Resume from exact stopping point after interruptions
- No duplicate processing or API calls
- File locking prevents concurrent runs

## Sync Pipeline for Storage

The sync pipeline automatically downloads all converted books from GRIN to your chosen storage backend with comprehensive database tracking.

### Quick Start

```bash
# Sync all converted books to Cloudflare R2
python sync.py pipeline output/harvard_2024/books.db --storage=r2 --bucket=grin-raw

# Check sync status
python sync.py status output/harvard_2024/books.db

# Retry failed syncs
python sync.py pipeline output/harvard_2024/books.db --storage=r2 --bucket=grin-raw --status=failed
```

### Sync Pipeline Features

- **Concurrent Downloads**: Download multiple books simultaneously (default: 3)
- **Database Tracking**: Complete sync status tracking in SQLite database
- **Resume Capability**: Automatically resumes from interruptions
- **Progress Monitoring**: Real-time progress with ETA calculations
- **Error Recovery**: Retry failed downloads with `--status=failed`
- **Duplicate Detection**: Uses Google ETags to avoid redundant downloads
- **Parallel Processing**: Saves encrypted archive, timestamp, and decrypted version simultaneously

### Sync Status Tracking

The database tracks comprehensive sync metadata for each book:
- **Storage Type**: Target storage backend (r2, minio, s3, local)
- **Storage Paths**: Both encrypted and decrypted archive locations
- **ETag Verification**: Google's ETag for duplicate detection
- **Sync Status**: pending, syncing, completed, failed
- **Decryption Status**: Whether decrypted version exists
- **Error Tracking**: Detailed error messages for failed syncs

```bash
# Check overall sync status
python sync.py status output/harvard_2024/books.db

# Check status for specific storage type
python sync.py status output/harvard_2024/books.db --storage-type=r2
```

### Sync Pipeline Options

```bash
# Basic sync to R2
python sync.py pipeline output/harvard_2024/books.db --storage=r2 --bucket=grin-raw

# Sync with custom concurrency and batch size
python sync.py pipeline output/harvard_2024/books.db --storage=r2 --bucket=grin-raw --concurrent=5 --batch-size=50

# Sync limited number of books
python sync.py pipeline output/harvard_2024/books.db --storage=r2 --bucket=grin-raw --limit=1000

# Force re-sync (overwrite existing files)
python sync.py pipeline output/harvard_2024/books.db --storage=r2 --bucket=grin-raw --force

# Retry only failed syncs
python sync.py pipeline output/harvard_2024/books.db --storage=r2 --bucket=grin-raw --status=failed
```

## Download Individual Books

Download specific book archives from GRIN. **Note**: Only books in "converted" state have downloadable archives.

```bash
# Download a single book archive (must be in converted state)
python download.py TZ1XH8

# Download to specific directory
python download.py TZ1XH8 -o ./downloads

# Download to S3
python download.py TZ1XH8 --storage=s3 --bucket=grin-raw

# Download to MinIO (auto-configured from docker-compose.minio.yml)
python download.py TZ1XH8 --storage=minio --bucket=grin-raw

# Download to Cloudflare R2 (uses ~/.config/grin-to-s3/r2_credentials.json)
python download.py TZ1XH8 --storage=r2 --bucket=grin-raw

# Download to R2 with custom credentials file
python download.py TZ1XH8 --storage=r2 --bucket=grin-raw --credentials-file=~/my-r2-creds.json

# Force download and overwrite existing files (skip ETag check)
python download.py TZ1XH8 --force

# Use custom GPG key file for decryption
python download.py TZ1XH8 --gpg-key-file=~/my-gpg-key.asc
```

### Finding Downloadable Books

To find books that are actually available for download:

1. **Check the enriched database** for books with `grin_state = "converted"`
2. **Use the collection system** to identify converted books:

```bash
# Collect books and check their states
python collect_books.py --run-name "find_converted" --limit 1000
python grin_enrichment.py enrich output/find_converted/books.db --limit 1000
python grin_enrichment.py export-csv output/find_converted/books.db --output converted_books.csv

# Filter CSV for books with converted state
grep ",converted," converted_books.csv | head -10
```

### Download Workflow

The download script automatically:
1. **Verifies converted state**: Checks if book is in GRIN's `_converted` endpoint
2. **Confirms file availability**: Uses HEAD request to verify archive exists and get Google's ETag
3. **Shows download progress**: Displays percentage complete when file size is known
4. **Checks for duplicates**: Compares Google's ETag with stored metadata to skip identical files
5. **Downloads encrypted archive**: Retrieves `.tar.gz.gpg` file (typically 20-80 MB) only if needed
6. **Parallel async uploads**: Saves encrypted archive, timestamp, and decrypted version simultaneously using non-blocking operations
7. **Reports completion**: Shows paths for both encrypted and decrypted archives after upload

**Supported Storage Backends:**
- Local filesystem (default)
- AWS S3
- MinIO (S3-compatible) - auto-configured from `docker-compose.minio.yml`
- Cloudflare R2

**MinIO Setup:**
```bash
# Start MinIO locally
docker-compose -f docker-compose.minio.yml up -d

# Download to MinIO (auto-configured, will offer to create bucket if it doesn't exist)
python download.py TZ1XH8 --storage=minio --bucket=grin-raw

# Access MinIO console: http://localhost:9001 (minioadmin/minioadmin123)
```

**File Formats**: 
- **Encrypted**: Original `.tar.gz.gpg` files from Google (used for ETag comparison)
- **Decrypted**: Ready-to-use `.tar.gz` archives containing book page images and metadata

**Duplicate Detection**: For S3-compatible storage, the download system uses Google's native ETags to avoid redundant downloads. When a file already exists with the same ETag, the download is skipped entirely, saving bandwidth and time.

**Force Mode**: Use `--force` to skip ETag checks and overwrite existing files. This forces a fresh download regardless of whether the file already exists.

**GPG Requirements**: The system requires `gpg` to be installed and configured for automatic decryption. GPG keys and passphrases can be provided in several ways:

1. **Default locations**: 
   - Place your GPG key at `~/.config/grin-to-s3/gpg_key.asc`
   - Place your passphrase at `~/.config/grin-to-s3/gpg_passphrase.asc` (if key is passphrase-protected)
2. **Custom file**: Use `--gpg-key-file` to specify a different key file location
3. **System keyring**: Import keys manually with `gpg --import <key_file>`

The passphrase file should contain only the passphrase text (no encryption). The system will throw an error if the passphrase file is missing or empty when required. If GPG is not installed or keys are not configured, the system will show a warning and skip decryption while still saving the encrypted archive successfully.

## Performance Characteristics

**Collection Stage:**
- **Rate Limiting**: Respects 5 QPS API limits 
- **Pagination**: 5,000 books per API request for efficiency
- **Concurrent Safety**: File locking prevents duplicate runs
- **Memory Efficient**: Streaming processing with bounded memory usage

**Enrichment Stage:**
- **Dynamic Batching**: Maximizes URL capacity (~1000+ barcodes per request)
- **Concurrent Processing**: Up to 5 simultaneous API requests with rate limiting
- **High Throughput**: Processes thousands of books per minute
- **Progress Tracking**: Real-time progress with ETA calculations after initial batches

## Key Features

- **Async architecture** with Python 3.12+
- **Minimal dependencies** for easy installation
- **Resume functionality** for interrupted operations
- **Comprehensive logging** and progress tracking
- **Easy installation** for other libraries

## Architecture

### Core Modules
- `auth.py` - OAuth2 authentication with google-auth
- `client.py` - Async GRIN API client with aiohttp  
- `common.py` - Shared utilities and progress tracking
- `storage.py` - S3/MinIO/R2 storage abstractions

### Book Collection Module (`collect_books/`)
- `models.py` - BookRecord and RateLimiter data models
- `exporter.py` - Main BookCollector class and collection logic
- `config.py` - Configuration management and pagination settings
- `__main__.py` - CLI interface (`python -m collect_books`)

### Top-Level Scripts
- `collect_books.py` - Convenient wrapper script for book collection
- `grin_enrichment.py` - GRIN metadata enrichment pipeline

## Dependencies

Modern Python 3.12+ with curated dependencies:
- `aiohttp` - Async HTTP client
- `google-auth` - OAuth2 authentication  
- `aiosqlite` - Async SQLite database operations
- `selectolax` - Fast HTML parsing for GRIN responses
- `aioboto3` - Async AWS operations
- `aiofiles` - Async file I/O
- `fsspec`/`s3fs`/`gcsfs` - Storage abstraction
- `tenacity` - Retry logic
- `pytest` - Testing framework

## Development

### Code Quality Tools

The project includes modern development tools (install with `pip install -e ".[dev]"`):

```bash
# Run linting (fixes most issues automatically)
ruff check --fix

# Run code formatting (Ruff formatter, compatible with Black)
ruff format

# Run type checking
mypy --explicit-package-bases *.py

# Run all quality checks
ruff check --fix && ruff format && mypy --explicit-package-bases *.py
```

### Testing

Run the test suite to verify functionality:

```bash
# Run all tests
python -m pytest

# Run book collection tests only
python -m pytest tests/ -k "collect_books"

# Run with verbose output
python -m pytest tests/unit/test_collect_books.py -v
```

## Troubleshooting

### Common Issues

**Authentication Error**: 
```bash
python auth.py setup  # Re-run OAuth2 setup
```

**Module Not Found**:
```bash
source venv/bin/activate  # Ensure virtual environment is active
pip install -e ".[dev]"  # Install dependencies in development mode
```

**Permission Denied**:
```bash
chmod +x activate.sh  # Make activation script executable
```

**Book Collection Fails**:
```bash
# Try test mode first
python collect_books.py --test-mode --limit 10

# Check status of existing collection
python grin_enrichment.py status output/default/books.db
```
