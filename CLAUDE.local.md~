# CLAUDE.md

## Important Instructions
- Never use emojis in code, output, or communication
- Use direct, technical language only
- No enthusiasm or exclamation marks
- State facts without embellishment
- Show the user commit messages and allow the user to edit them in VSCode before commiting
- This project has no existing users, so don't try to maintain backwards compatibility with changes unless instructed
- Barcodes are just random strings. Don't look for patterns in them
- the v1 code is in  ~/work/harvard-idi/grin-to-s3-internal
- we should expect that this collection has already been processed many times
- never write commit messages that reference code quality unless that's the only change in the commit

## Project Overview

This is a data pipeline for archiving Google Books data from GRIN (Google Return INterface) to S3-like storage on behalf of academic libraries.

### History

This is a rewrite of an earlier version of the project that did not meet performance or ease-of-use goals.

### Core Outputs
1. **Decrypted tarballs** - All original `barcode.tar.gz` files (decrypted, not .gpg)
2. **JSON text files** - `barcode.json` with page array: `["page 1 text", "page 2 text"]`  
3. **Single CSV** - All books with Google API fields + METS bibliographic data only
4. **Process log** - Single summary file with timing, errors, retries, interruptions

### Architecture Goals
- CLI tool focused on extraction
- Easy to install, use, and share with other libraries
- Maintain robustness (interrupt/resume, updates without overwriting)
- Maintain rate limiting (5 QPS, 50K daily conversions, 50K max queue)
- Comprehensive sync tracking with SQLite database
- Optional HathiTrust rights matching as stretch goal

### Development Guidelines
- Modern async/await architecture with aiohttp and aiofiles
- Database schema maintained in `docs/schema.sql` for consistency
- Always check tests and types before committing
- Always run ruff before committing too

## Common Development Commands

### Testing and Type Checking
```bash
# Run tests
pytest

# Type checking
mypy src/grin_to_s3/*.py --explicit-package-bases
```

### Running the Pipeline
```bash
# Collect books (requires three-bucket storage config, saves configuration for other scripts)
python grin.py collect --run-name harvard_2024 --storage r2 \
  --bucket-raw grin-raw --bucket-meta grin-meta --bucket-full grin-full

# Request book processing (uses config from run directory)
python grin.py process request --run-name harvard_2024 --limit 1000

# Monitor processing status
python grin.py process monitor --run-name harvard_2024

# Sync converted books to storage (uses storage config from run directory)
python grin.py sync pipeline --run-name harvard_2024

# Check sync status
python grin.py sync status --run-name harvard_2024

# Download already-converted books from GRIN
python grin.py sync catchup --run-name harvard_2024

# Enrich with detailed metadata (uses config from run directory)
python grin.py enrich --run-name harvard_2024

# Export to CSV
python grin.py export-csv --run-name harvard_2024 --output books.csv
```

## Architecture Overview

### Storage Structure

The system uses three separate buckets for different data types:

**Raw Data Bucket** (original archives):
```
bucket-raw/BARCODE/
├── BARCODE.tar.gz.gpg           # Original encrypted archive
└── BARCODE.tar.gz.gpg.retrieval # Timestamp file
```

**Metadata Bucket** (CSV and database outputs):
```
bucket-meta/
├── books.csv                    # Combined book metadata
├── processing_log.csv           # Processing status log
└── sync_status.csv              # Sync tracking data
```

**Full-text Bucket** (OCR and text extraction):
```
bucket-full/BARCODE/
├── BARCODE.txt                  # Compiled full text
├── BARCODE.txt.tar.gz           # Individual page texts
├── BARCODE.xml                  # MARC XML from Google
└── BARCODE.json                 # Structured text data
```

### Important Notes
- GRIN conversion rate limit: 5,000 books/day by default
- Archives must be downloaded before being replaced by newly converted ones
- Processing large corpora (millions of books) takes months

## Workflow Memories


## Current TODO List

### High Priority - Pending
- **Local storage optimization**: For local storage, require the file path, don't have a default. Modify sync to skip the staging and upload path completely, and just write the files directly to the local storage path
- **E2E test workflow**: Add e2e-test workflow that runs complete pipeline lifecycle with small limit (10 books): collect -> process request -> process monitor -> sync pipeline -> enrich -> export-csv. Should validate entire workflow end-to-end with real or mock data
- **Split out linting from pytest in workflow file**: Separate ruff/mypy linting checks from pytest execution in CI workflow for better visibility and faster feedback
- **OCR text export feature**: Add feature to output all of the collection's OCR plain text exports, grouped by barcode and indexed by page. Output format: barcode.json containing ["page 1 text", "page 2 text", "page 3 text", ...]
- **ETag optimization**: Replace encrypted file uploads with ETag metadata storage on bucket objects. Use original GRIN ETag as metadata to check for changes before syncing, eliminating need to store encrypted files
- **Comprehensive CSV export**: Create single CSV file listing all books in collection with complete metadata including all GRIN fields (per spec doc) and all bibliographic data from METS files (derived from MARC records). Should include books not yet fully processed by pipeline
- **Docker image**: Create ready-to-use Docker image for grin-to-s3 with all dependencies and proper configuration

### Medium Priority - Pending  
- **Refactor logging setup**: Refactor duplicated logging setup pattern (4+ files: grin_enrichment.py:590-630, processing.py:988-1023, sync.py:1056-1090, collect_books/__main__.py:33-92) into shared utility
- **Refactor database validation**: Consolidate duplicated database validation logic (3 files: grin_enrichment.py:632-691, processing.py:1025-1052, sync.py:1092-1123) into single utility function
- **Unified logging option**: Add global option for single log file vs split logs per command. Default to single log file, current behavior is split logs. Ensure appending to existing log works correctly
- **Preflight recommendations**: Add preflight command that accepts all entry point options (storage, buckets, concurrency, etc.) and makes performance/configuration recommendations. Examples: warn about r2.dev bucket upload concurrency, suggest optimal staging directory size, validate storage connectivity

### Low Priority - Pending
- **Merge bucket functions**: Merge duplicate bucket creation functions in download.py (ensure_bucket_exists_with_storage and ensure_bucket_exists - 95% identical code)
- **Unify rate limiter**: Create unified rate limiter class to replace various implementations across grin_enrichment.py:96-121, processing.py:53-78, collect_books/models.py:159-184

### Completed (Recent)
- Staging directory management system with disk space monitoring
- Disk space checking that pauses downloads at 90% capacity
- Modified download pipeline to save files to staging directory instead of memory
- Updated upload logic to stream from staging directory files
- Implemented immediate cleanup of staging files after both uploads complete
- Added startup check for existing staging directory files
- Ensured staging directory persistence across script restarts
- Handled stale/orphaned files in staging directory from interrupted runs